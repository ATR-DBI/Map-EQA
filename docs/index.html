<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Map-based Modular Approach for Zero-shot Embodied Question Answering</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  
  
  <section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Map-based Modular Approach for Zero-shot Embodied Question Answering</h1>
                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                            <a href="">Koya Sakamoto</a>,
                        </span>
                        <span class="author-block">
                            <a href="https://dachii-azm.github.io/">Daichi Azuma</a>,
                        </span>
                        <span class="author-block">
                            <a href="https://miyatai.org/">Taiki Miyanishi</a>,
                        </span>
                        <span class="author-block">
                            <a href="https://shuheikurita.github.io/">Shuhei Kurita</a>,
                        </span>
                        <span class="author-block">
                            <a href="https://bicr.atr.jp/~kawanabe/">Motoaki Kawanabe</a>,
                        </span>
                    </div>
                    <!-- <div class="is-size-5 publication-authors">
                        <span class="author-block">Work done at Kyoto University,.</span>
                    </div> -->

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <span class="link-block">
                                <a target="_blank" href="https://arxiv.org/abs/2405.16559" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>Paper</span>
                                </a>
                                <a target="_blank" href="https://github.com/ATR-DBI/Map-EQA" class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                                    </span>
                                    <span>Code</span>
                                </a>
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<hr class="container is-max-desktop separator">

<!-- Teaser video-->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <div class="content has-text-justified">
          <p><b>TL;DR</b>: Our paper introduces a map-based modular approach to Embodied Question Answering (EQA), 
            enabling real-world robots to explore while answering a wide range of natural language questions, with demonstrated effectiveness in both virtual and real-world settings.</p>
            <p class="new-line"></p>
          <img src="static/figures/overview.png" alt="teaser" class="blend-img-background center-image" class="blend-img-background center-image">
        </div>
      </div>
    </div>
  </div>
</section>

<hr class="container is-max-desktop separator">

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <source src="static/videos/iros_2024_map_eqa_sup_movie_high_quality.mp4"
            type="video/mp4">
          </video>
          <p>
            Embodied Question Answering (EQA) serves as a benchmark task to evaluate the capability of robots to navigate within novel environments and identify objects in response to human queries. 
            However, existing EQA methods often rely on simulated environments and operate with limited vocabularies. 
            This paper presents a map-based modular approach to EQA, enabling real-world robots to explore and map unknown environments. 
            By leveraging foundation models, our method facilitates answering a diverse range of questions using natural language. 
            We conducted extensive experiments in both virtual and real-world settings, demonstrating the robustness of our approach in navigating and comprehending queries within unknown environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<hr class="container is-max-desktop separator">

<!-- Preprocessing -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Data Preprocess</h2>
        <div class="content has-text-justified">
          <img src="static/figures/preprocess.png" alt="prompts for data preprocessing" class="blend-img-background center-image">
          <p class="new-line"></p>
          <p>
            Dataset pre-processing using gpt-35-turbo-0613. It extracts a target object category from a given question for ObjNav and converts a question into a declarative text for image-text matching.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<hr class="container is-max-desktop separator">

<!-- Method -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <img src="static/figures/overview_proposed_method_v2.png" alt="our proposed method" class="blend-img-background center-image">
          <p class="new-line"></p>
          <p>
            We propose map-based modular approach that combines effective off-the-shelf models to perform EQA.
            The proposed method comprises the Navigation module (outlined in blue) and the VQA module (outlined in red).
            The Navigation module consists of the Perception module and a set of Policies. 
            The Perception module incrementally builds a 2D map, storing images along with their image-text matching scores. 
            The Global Policy selects a long-term goal based on the 2D map and its frontiers. 
            The Deterministic Local Policy outputs actions, and finally, the VQA module provides an answer based on the memorized images and the given question.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<hr class="container is-max-desktop separator">

<!-- ITM -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Image-text Matching in MP3D-EQA</h2>
        <div class="content has-text-justified">
          <img src="static/figures/itm_scores_mp3d_train_train_v2.png" alt="figure of itm" class="blend-img-background center-image">
          <p class="new-line"></p>
          <p>
            The EQA agent has to distinguish target objects from others based on a declarative text converted from a question.
            To tackle this problem, we use vision-language foundation models BLIP2 and CLIP as an image-text matching module. 
            The result in MP3D-EQA shows that the pair of BLIP2 and declarative text is the best among the others.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<hr class="container is-max-desktop separator">

<!-- VQA -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">VQA Results in MP3D-EQA</h2>
        <div class="content has-text-justified">
          <img src="static/figures/vqa_scores_mp3d_train_train.png" alt="figure of vqa" class="blend-img-background center-image">
          <p class="new-line"></p>
          <p>
            We investigate which VQA models are effective for MP3D-EQA. The result indicates that LLaVA is the most effective, so we adopt LLaVA as a VQA module.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<hr class="container is-max-desktop separator">

<!-- Simulation Results -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">EQA Results in Simulation</h2>
        <div class="content has-text-justified">
          <img src="static/figures/eqa_results_simulation.png" alt="eqa results" class="blend-img-background center-image">
          <p class="new-line"></p>
          <p>
            The results of EQA in MP3D-EQA highlight our method consistently outperforms the VQA-only baseline. 
            It indicates that the navigation module of our method can work efficiently to answer questions.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<hr class="container is-max-desktop separator">

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">EQA Results in Simulation</h2>
      <div class="custom_slider w-slider" data-autoplay="true" data-duration="800">
        <!-- スライドを包むマスクエリア -->
        <div class="mask w-slider-mask" id="w-slider-mask-0">
          
          <!-- 各スライド -->
          <div class="slide w-slide" aria-label="1 of 7" role="group">
            <div class="div-block-9 first_video">
              <div class="video_class w-embed">
                <video width="100%" height="auto" autoplay muted controls loop preload="metadata">
                  <source src="static/videos/movie_1.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
          </div>

          <div class="slide w-slide" aria-label="2 of 7" role="group">
            <div class="div-block-9">
              <div class="video_class w-embed">
                <video width="100%" height="auto" autoplay muted controls loop preload="metadata">
                  <source src="static/videos/movie_2.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
          </div>

          <div class="slide w-slide" aria-label="3 of 7" role="group">
            <div class="div-block-9">
              <div class="video_class w-embed">
                <video width="100%" height="auto" autoplay muted controls loop preload="metadata">
                  <source src="static/videos/movie_3.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
          </div>
          <div class="slide w-slide" aria-label="4 of 7" role="group">
            <div class="div-block-9">
              <div class="video_class w-embed">
                <video width="100%" height="auto" autoplay muted controls loop preload="metadata">
                  <source src="static/videos/movie_4.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
          </div>
          <div class="slide w-slide" aria-label="5 of 7" role="group">
            <div class="div-block-9">
              <div class="video_class w-embed">
                <video width="100%" height="auto" autoplay muted controls loop preload="metadata">
                  <source src="static/videos/movie_5.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
          </div>
          <div class="slide w-slide" aria-label="6 of 7" role="group">
            <div class="div-block-9">
              <div class="video_class w-embed">
                <video width="100%" height="auto" autoplay muted controls loop preload="metadata">
                  <source src="static/videos/movie_6.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
          </div>
          <div class="slide w-slide" aria-label="7 of 7" role="group">
            <div class="div-block-9">
              <div class="video_class w-embed">
                <video width="100%" height="auto" autoplay muted controls loop preload="metadata">
                  <source src="static/videos/movie_7.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
          </div>
        </div>
        <!-- 矢印 -->
        <div class="arrow left-arrow" onclick="previousSlide()">&#9664;</div>
        <div class="arrow right-arrow" onclick="nextSlide()">&#9654;</div>
      </div>
    </div>
  </div>
  <div class="dots-container is-max-desktop"></div>
</div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{sakamoto2024mapeqa,
          author={Koya Sakamoto, Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, Motoaki Kawanabe},
          title={Map-based Modular Approach for Zero-shot Embodied Question Answering},
          booktitle={Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
          year={2024},
        }
  </code></pre>
  </div>
</section>

<script>
  let currentSlide = 0; // 現在のスライドのインデックス
  const slides = document.querySelectorAll('.w-slide'); // 全スライドを取得
  const totalSlides = slides.length; // スライドの総数

  const sliderMask = document.getElementById('w-slider-mask-0'); // マスク部分を取得

  function nextSlide() {
    currentSlide = (currentSlide + 1) % totalSlides; // 次のスライドに移動
    updateSlidePosition();
  }

  function previousSlide() {
    currentSlide = (currentSlide - 1 + totalSlides) % totalSlides; // 前のスライドに移動
    updateSlidePosition();
  }

  function updateSlidePosition() {
    const newTransformValue = `translateX(-${currentSlide * 100}%)`;
    sliderMask.style.transform = newTransformValue; // スライド位置を更新
    
    updateDots(); // ドットの状態を更新
  }

  function createDots() {
    const dotsContainer = document.querySelector('.dots-container');
    for (let i = 0; i < totalSlides; i++) {
      const dot = document.createElement('div');
      dot.classList.add('dot');
      dot.addEventListener('click', () => {
        currentSlide = i; // クリックされたインデックスを設定
        updateSlidePosition(); // スライドを更新
      });
      dotsContainer.appendChild(dot);
    }
  }

  function updateDots() {
    const dots = document.querySelectorAll('.dot');
    dots.forEach((dot, index) => {
      dot.classList.toggle('active', index === currentSlide);
    });
  }

  // 初期化
  createDots();
  updateDots();
  // 自動再生の設定（10秒ごとに次のスライドへ）
  setInterval(() => {
    nextSlide();
  }, 10000); // 10000ms = 10秒
</script>




<style>
  .container-wrapper {
    display: flex;
    justify-content: center;
    align-items: center;
    /* min-height: 100vh; */
  }

  .custom-width {
    width: 50%;
  }
</style>

<style>
  .enlarged-image {
    width: 120%; /* 画像の幅をコンテナに合わせる */
    height: 120%; /* 画像の縦横比を維持 */
  }
</style>

<style>
  .center-image {
    display: block;
    margin: 0 auto;
  }
</style>

<style>
  .publication-links a {
    margin-right: 50px; /* アイコン間の隙間を調整 */
  }

  /* 最後の要素には右側のマージンを追加しない */
  .publication-links a:last-child {
    margin-right: 0;
  }
</style>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
